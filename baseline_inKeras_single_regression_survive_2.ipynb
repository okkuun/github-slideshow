{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_inKeras_single_regression_survive_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okkuun/github-slideshow/blob/main/baseline_inKeras_single_regression_survive_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PMLrNvjp9j_",
        "outputId": "e79fc520-6b50-4f39-e09a-7f6a94c66689"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  1 14:00:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GwN48j70Gf4",
        "outputId": "019f44d2-13d0-4eb7-ccd6-c06af5145c0b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJI_lyr80Orc",
        "outputId": "ed6c5170-52e0-4fe1-a567-e4260a0179fa"
      },
      "source": [
        "%%bash\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "cp -f ./drive/MyDrive/kaggle/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "pip install -U kaggle\n",
        "pip install wandb -qq\n",
        "pip install tensorflow-addons\n",
        "\n",
        "kaggle competitions download -c ventilator-pressure-prediction\n",
        "unzip sample_submission.csv.zip\n",
        "unzip test.csv.zip\n",
        "unzip train.csv.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            "\n",
            "Downloading test.csv.zip to /content\n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0.00/8.50M [00:00<?, ?B/s]\r100%|##########| 8.50M/8.50M [00:00<00:00, 77.9MB/s]\n",
            "\r  0%|          | 0.00/139M [00:00<?, ?B/s]\r  7%|7         | 10.0M/139M [00:00<00:01, 100MB/s]\r 17%|#6        | 23.0M/139M [00:00<00:01, 108MB/s]\r 27%|##6       | 37.0M/139M [00:00<00:00, 115MB/s]\r 36%|###5      | 50.0M/139M [00:00<00:00, 120MB/s]\r 47%|####6     | 65.0M/139M [00:00<00:00, 127MB/s]\r 57%|#####6    | 79.0M/139M [00:00<00:00, 132MB/s]\r 65%|######5   | 91.0M/139M [00:00<00:00, 126MB/s]\r 74%|#######4  | 103M/139M [00:00<00:00, 95.3MB/s]\r 82%|########1 | 114M/139M [00:01<00:00, 98.3MB/s]\r 89%|########9 | 124M/139M [00:01<00:00, 97.0MB/s]\r 96%|#########6| 134M/139M [00:01<00:00, 97.0MB/s]\r100%|##########| 139M/139M [00:01<00:00, 108MB/s] \n",
            "\r  0%|          | 0.00/75.4M [00:00<?, ?B/s]\r  8%|7         | 6.00M/75.4M [00:00<00:01, 59.7MB/s]\r 32%|###1      | 24.0M/75.4M [00:00<00:00, 75.0MB/s]\r 54%|#####4    | 41.0M/75.4M [00:00<00:00, 81.9MB/s]\r 76%|#######5  | 57.0M/75.4M [00:00<00:00, 95.8MB/s]\r 95%|#########5| 72.0M/75.4M [00:00<00:00, 107MB/s] \r100%|##########| 75.4M/75.4M [00:00<00:00, 127MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy_apR9B0RL7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import optuna\n",
        "\n",
        "import os \n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize, QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import yaml\n",
        "import gc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNOOE2Io0Zfw"
      },
      "source": [
        "# making config\n",
        "\n",
        "cfg = yaml.safe_load(\n",
        "    \"\"\"\n",
        "    globals:\n",
        "        name: keras_single_11\n",
        "        seed: 2021\n",
        "\n",
        "    features:\n",
        "        all_features_cat: []\n",
        "        all_features_con: []\n",
        "        all_features: []\n",
        "\n",
        "    model:\n",
        "        hidden_size: 1024\n",
        "        epochs: 300\n",
        "        drop_rate: 0.2\n",
        "        batch_size: 1024\n",
        "\n",
        "    optimizer:\n",
        "        lr: 1.0e-3\n",
        "        weight_decay: 1.0e-6\n",
        "        max_grad_norm: 1000\n",
        "        min_lr: 1.0e-5\n",
        "        scheduler: ReduceLROnPlateau\n",
        "    \"\"\"\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK39WYlPZ5Ba"
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    gc.collect()\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlxB8AtWoYu"
      },
      "source": [
        "# [TODO] 特徴量の限定（過学習の一因）\n",
        "# [TODO] 二乗を混ぜるように工夫\n",
        "def add_features(df):\n",
        "    df['u_out_formodel'] = df['u_out'].copy()\n",
        "    df['cross']= df['u_in'] * df['u_out']\n",
        "    df['cross2']= df['time_step'] * df['u_out']\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    _delta_time = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    for i_shift in range(1, 1+1):\n",
        "        df[f'delta_u_in_{i_shift}'] = df[f'u_in_diff{i_shift}'] / np.where(_delta_time>=0.03*i_shift, _delta_time, 0.03*i_shift)\n",
        "\n",
        "    df['area_gap'] = df['u_in'] * _delta_time\n",
        "    df['area_cumsum_gap'] = (df['area_gap']).groupby(df['breath_id']).cumsum()\n",
        "\n",
        "    df['ewm_u_in_mean'] = (df\\\n",
        "                           .groupby('breath_id')['u_in']\\\n",
        "                           .apply(lambda x: x.ewm(halflife=9)\\\n",
        "                           .mean())\\\n",
        "                           .reset_index(level=0,drop=True))\n",
        "    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n",
        "                                                              .groupby('breath_id')['u_in']\\\n",
        "                                                              .apply(lambda x: x.rolling(window=15,min_periods=1)\\\n",
        "                                                              .agg({\"15_in_sum\":\"sum\",\n",
        "                                                                    \"15_in_min\":\"min\",\n",
        "                                                                    \"15_in_max\":\"max\",\n",
        "                                                                    \"15_in_mean\":\"mean\"}))\\\n",
        "                                                               .reset_index(level=0,drop=True))\n",
        "    print(\"Step-6...Completed\")\n",
        "\n",
        "    # df = reduce_mem_usage(df)\n",
        "    \n",
        "    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n",
        "    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n",
        "    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n",
        "    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n",
        "    print(\"Step-7...Completed\")\n",
        "\n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-8...Completed\")\n",
        "\n",
        "    for c in df.columns:\n",
        "        if c==\"u_out\":\n",
        "            df[c] = df[c].astype(\"uint8\")\n",
        "        elif df[c].dtype==\"float64\":\n",
        "            df[c] = df[c].astype(\"float32\")\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBKXlRSOoFE_"
      },
      "source": [
        "\n",
        "# なるべくu_inではなくcrossを使う\n",
        "# [TODO] u_inやu_out系の特徴量もあった方が精度良いか？\n",
        "def add_features_2(df):\n",
        "    df['u_out_formodel'] = df['u_out'].copy()\n",
        "    df['cross']= df['u_in'] * (1 - df['u_out'])\n",
        "    df['cross2']= df['time_step'] * (1 - df['u_out'])\n",
        "    # df['area'] = df['time_step'] * df['u_in']\n",
        "    # df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['area2'] = df['time_step'] * df['cross']\n",
        "    df['area2'] = df.groupby('breath_id')['area2'].cumsum()\n",
        "    # df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
        "    df['cross2_cumsum'] = df.groupby(['breath_id'])['cross2'].cumsum()\n",
        "    # df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    df['cross_cumsum'] = (df['cross']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['cross_lag1'] = df.groupby('breath_id')['cross'].shift(1)\n",
        "    # df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['cross_lag_back1'] = df.groupby('breath_id')['cross'].shift(-1)\n",
        "    # df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['cross_lag2'] = df.groupby('breath_id')['cross'].shift(2)\n",
        "    # df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['cross_lag_back2'] = df.groupby('breath_id')['cross'].shift(-2)\n",
        "    # df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['cross_lag3'] = df.groupby('breath_id')['cross'].shift(3)\n",
        "    # df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['cross_lag_back3'] = df.groupby('breath_id')['cross'].shift(-3)\n",
        "    # df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['cross_lag4'] = df.groupby('breath_id')['cross'].shift(4)\n",
        "    # df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['cross_lag_back4'] = df.groupby('breath_id')['cross'].shift(-4)\n",
        "    # df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    # df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    # df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n",
        "    # df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    # df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__cross__max'] = df.groupby(['breath_id'])['cross'].transform('max')\n",
        "    df['breath_id__cross__mean'] = df.groupby(['breath_id'])['cross'].transform('mean')\n",
        "    df['breath_id__cross__diffmax'] = df.groupby(['breath_id'])['cross'].transform('max') - df['cross']\n",
        "    df['breath_id__cross__diffmean'] = df.groupby(['breath_id'])['cross'].transform('mean') - df['cross']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    # df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    # df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    # df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    # df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    # df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    # df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    # df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    # df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    df['cross_diff1'] = df['cross'] - df['cross_lag1']\n",
        "    df['cross_diff2'] = df['cross'] - df['cross_lag2']\n",
        "    df['cross_diff3'] = df['cross'] - df['cross_lag3']\n",
        "    df['cross_diff4'] = df['cross'] - df['cross_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    # df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    df['cross_cummean'] = df['cross_cumsum'] / df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    # df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    # df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    # df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    # df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    df['breath_id__cross_lag'] = df['cross'].shift(1).fillna(0)\n",
        "    df['breath_id__cross_lag'] = df['breath_id__cross_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__cross_lag2'] = df['cross'].shift(2).fillna(0)\n",
        "    df['breath_id__cross_lag2'] = df['breath_id__cross_lag2'] * df['breath_id_lag2same']\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    _delta_time = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    for i_shift in range(1, 4+1):\n",
        "        # df[f'delta_u_in_{i_shift}'] = df[f'u_in_diff{i_shift}'] / np.where(_delta_time>=0.03*i_shift, _delta_time, 0.03*i_shift)\n",
        "        df[f'delta_cross_{i_shift}'] = df[f'cross_diff{i_shift}'] / np.where(_delta_time>=0.03*i_shift, _delta_time, 0.03*i_shift)\n",
        "\n",
        "    # df['ewm_u_in_mean'] = (df\\\n",
        "    #                        .groupby('breath_id')['u_in']\\\n",
        "    #                        .apply(lambda x: x.ewm(halflife=9)\\\n",
        "    #                        .mean())\\\n",
        "    #                        .reset_index(level=0,drop=True))\n",
        "    # df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n",
        "    #                                                           .groupby('breath_id')['u_in']\\\n",
        "    #                                                           .apply(lambda x: x.rolling(window=15,min_periods=1)\\\n",
        "    #                                                           .agg({\"15_in_sum\":\"sum\",\n",
        "    #                                                                 \"15_in_min\":\"min\",\n",
        "    #                                                                 \"15_in_max\":\"max\",\n",
        "    #                                                                 \"15_in_mean\":\"mean\"}))\\\n",
        "    #                                                            .reset_index(level=0,drop=True))\n",
        "    df['ewm_cross_mean'] = (df\\\n",
        "                           .groupby('breath_id')['cross']\\\n",
        "                           .apply(lambda x: x.ewm(halflife=9)\\\n",
        "                           .mean())\\\n",
        "                           .reset_index(level=0,drop=True))\n",
        "    df[[\"15_cross_sum\",\"15_cross_min\",\"15_cross_max\",\"15_cross_mean\"]] = (df\\\n",
        "                                                              .groupby('breath_id')['cross']\\\n",
        "                                                              .apply(lambda x: x.rolling(window=10,min_periods=1)\\\n",
        "                                                              .agg({\"15_cross_sum\":\"sum\",\n",
        "                                                                    \"15_cross_min\":\"min\",\n",
        "                                                                    \"15_cross_max\":\"max\",\n",
        "                                                                    \"15_cross_mean\":\"mean\"}))\\\n",
        "                                                               .reset_index(level=0,drop=True))\n",
        "    print(\"Step-6...Completed\")\n",
        "    \n",
        "    # df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n",
        "    # df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n",
        "    # df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n",
        "    # df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n",
        "    df['cross_lagback_diff1'] = df['cross'] - df['cross_lag_back1']\n",
        "    df['cross_lagback_diff2'] = df['cross'] - df['cross_lag_back2']\n",
        "    print(\"Step-7...Completed\")\n",
        "\n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-8...Completed\")\n",
        "\n",
        "    for c in df.columns:\n",
        "        if c==\"u_out\":\n",
        "            df[c] = df[c].astype(\"uint8\")\n",
        "        elif df[c].dtype==\"float64\":\n",
        "            df[c] = df[c].astype(\"float32\")\n",
        "    return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gwd7Ss00nZJ",
        "outputId": "d695eef5-88ac-4455-d920-1a0c50974df9"
      },
      "source": [
        "# train = pd.read_csv('./train.csv')\n",
        "# test = pd.read_csv('./test.csv')\n",
        "# sample_sub = pd.read_csv('./sample_submission.csv')\n",
        "\n",
        "train = pd.read_csv('./drive/MyDrive/vpp/train2_pre.csv', index_col=0)\n",
        "test = pd.read_csv('./drive/MyDrive/vpp/test2_pre.csv', index_col=0)\n",
        "sample_sub = pd.read_csv('./sample_submission.csv')\n",
        "\n",
        "# # drop\n",
        "# train = train.drop(columns=drop_features)\n",
        "# test = test.drop(columns=drop_features)\n",
        "\n",
        "# u_inを非線形変換してみる\n",
        "# train['u_in'] = np.log1p(np.where(train['u_in'].values>0, train['u_in'].values, 0))\n",
        "# test['u_in'] = np.log1p(np.where(test['u_in'].values>0, test['u_in'].values, 0))\n",
        "\n",
        "# train = add_features_2(train)\n",
        "# # train = reduce_mem_usage(train)\n",
        "# test = add_features_2(test)\n",
        "# # test = reduce_mem_usage(test)\n",
        "\n",
        "# train.drop(['one','count',\n",
        "#             'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n",
        "#             'breath_id_lag2same'], axis=1, inplace=True)\n",
        "\n",
        "# test = test.drop(['one','count', 'breath_id_lag',\n",
        "#                   'breath_id_lag2','breath_id_lagsame',\n",
        "#                   'breath_id_lag2same'], axis=1)\n",
        "\n",
        "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
        "u_outs = train[['u_out']].to_numpy().reshape(-1, 80)\n",
        "\n",
        "feature_list = train.columns.tolist()\n",
        "for drop_c in ['pressure', 'id', 'breath_id']:\n",
        "    feature_list.remove(drop_c)\n",
        "\n",
        "feature_list.remove('u_out')\n",
        "cfg['features']['all_features'] = feature_list.copy()\n",
        "\n",
        "# 数値変数とカテゴリ変数で分けて変換\n",
        "feature_list_con = feature_list.copy()\n",
        "feature_list_cat = []\n",
        "for col in feature_list:\n",
        "    if len(train[col].unique()) <= 10:\n",
        "        feature_list_con.remove(col)\n",
        "        feature_list_cat.append(col)\n",
        "\n",
        "cfg['features']['all_features_con'] = feature_list_con\n",
        "cfg['features']['all_features_cat'] = feature_list_cat\n",
        "\n",
        "# QS = RobustScaler()\n",
        "# train.loc[:, feature_list_con] = QS.fit_transform(train.loc[:, feature_list_con])\n",
        "# test.loc[:, feature_list_con] = QS.transform(test.loc[:, feature_list_con])\n",
        "\n",
        "# # u_in系の特徴量にlogを適用する\n",
        "# for col in feature_list_con:\n",
        "#     str_list = col.split('_')\n",
        "    \n",
        "#     if 'u' in str_list and 'in' in str_list:\n",
        "#         train[col] = np.log1p(train[col])\n",
        "#         test[col] = np.log1p(test[col])\n",
        "\n",
        "#     elif 'cross' in str_list:\n",
        "#         train[col] = np.log1p(train[col])\n",
        "#         test[col] = np.log1p(test[col])\n",
        "\n",
        "# 数値変数の２乗を混ぜる\n",
        "# for col in feature_list:\n",
        "#     train[col+'_x2'] = train[col]**2\n",
        "#     test[col+'_x2'] = test[col]**2\n",
        "\n",
        "# memory reduction\n",
        "# train = reduce_mem_usage(train)\n",
        "# test = reduce_mem_usage(test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suRdDBaWC7ui"
      },
      "source": [
        "all_pressure = sorted(train.pressure.unique())\n",
        "# all_pressure_torch = torch.from_numpy(train.pressure.values).clone()\n",
        "P_MIN = np.min(all_pressure)\n",
        "P_MAX = np.max(all_pressure)\n",
        "P_STEP = all_pressure[1] - all_pressure[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGDp5Kh5We9H",
        "outputId": "7bde3ff6-d1a7-4958-f51c-5442e3dea26f"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    BATCH_SIZE = strategy.num_replicas_in_sync * 128\n",
        "    print(\"Running on TPU:\", tpu.master())\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "    \n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    BATCH_SIZE = 512\n",
        "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.9.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.9.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU: grpc://10.106.9.210:8470\n",
            "Batch Size: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmX43gKCWh2y"
      },
      "source": [
        "# from tensorflow.keras.layers import Bidirectional, LSTM\n",
        "# from tensorflow.keras.layers import Dense, Dropout, Input, LayerNormalization\n",
        "# from tensorflow.keras.layers import Concatenate, Add, GRU\n",
        "\n",
        "# def dnn_model():\n",
        "    \n",
        "#     x_input = Input(shape=(80, len(cfg['features']['all_features'])))\n",
        "#     x_emb = Dense(units=64, activation='selu',kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(x_input)\n",
        "#     x_emb = LayerNormalization(axis=-1)(x_emb)\n",
        "    \n",
        "#     x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_emb)\n",
        "#     x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "#     x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n",
        "    \n",
        "#     z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n",
        "#     z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n",
        "    \n",
        "#     x = Concatenate(axis=2)([x3, z2, z3])\n",
        "#     x = Bidirectional(LSTM(units=192, return_sequences=True))(x)\n",
        "    \n",
        "#     x = Dense(units=128, activation='selu',kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(x)\n",
        "#     x = LayerNormalization()(x)\n",
        "    \n",
        "#     x_output = Dense(units=1,kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(x)\n",
        "\n",
        "#     model = tf.keras.models.Model(inputs=x_input, outputs=x_output, name='DNN_Model')\n",
        "#     return model\n",
        "\n",
        "# model = dnn_model()\n",
        "# model.summary()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieyL0SP29Rei",
        "outputId": "8cbabd71-499b-419f-b8fa-57d20831771f"
      },
      "source": [
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LayerNormalization\n",
        "from tensorflow.keras.layers import Concatenate, Add, Multiply, BatchNormalization\n",
        "# # [TODO] 分類問題への発展（別ファイル？）\n",
        "def dnn_model():\n",
        "    x_input = Input(shape=(80, len(cfg['features']['all_features'])))\n",
        "    x_emb = Dense(units=64, activation='selu',kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(x_input)\n",
        "    x_emb = LayerNormalization(axis=1)(x_emb)\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = Dense(\n",
        "        units=128, activation='selu', \n",
        "        kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021)\n",
        "    )(x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    x_output = Dense(units=1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=x_input, outputs=x_output, \n",
        "                name='DNN_Model')\n",
        "    return model\n",
        "\n",
        "model = dnn_model()\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DNN_Model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 80, 53)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 80, 1536)     5050368     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 80, 1024)     8392704     bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 80, 768)      4328448     bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 80, 768)      3248640     bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 80, 768)      0           bidirectional_2[0][0]            \n",
            "                                                                 bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 80, 768)      3072        multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 80, 512)      2099200     bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 80, 512)      1575936     batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 80, 512)      0           bidirectional_3[0][0]            \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 512)      2048        multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 80, 256)      656384      bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 80, 256)      493056      batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 80, 256)      0           bidirectional_4[0][0]            \n",
            "                                                                 bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_8 (Bidirectional) (None, 80, 128)      123648      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 80, 1920)     0           bidirectional_4[0][0]            \n",
            "                                                                 bidirectional_5[0][0]            \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "                                                                 bidirectional_7[0][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 80, 128)      245888      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, 80, 128)      256         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 80, 1)        129         layer_normalization_1[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 26,220,801\n",
            "Trainable params: 26,217,729\n",
            "Non-trainable params: 3,072\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkU42fgyPFNn"
      },
      "source": [
        "# from tensorflow.keras import layers as L\n",
        "# from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
        "# from tensorflow.keras.layers import Dense, Dropout, Input, LayerNormalization\n",
        "# from tensorflow.keras.layers import Concatenate, Add, Multiply, BatchNormalization\n",
        "# def dnn_model(strategy):   \n",
        "#     with strategy.scope():\n",
        "#         inp = L.Input(shape=(80, len(cfg['features']['all_features'])),name=\"Input\")\n",
        "\n",
        "#         emb = Dense(units=64, activation='selu', kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(inp)\n",
        "#         emb = LayerNormalization(axis=-1)(emb)\n",
        "\n",
        "#         x1 = L.Bidirectional(L.LSTM(units=1024, return_sequences=True),name=\"BiLSTM1\")(emb)\n",
        "#         x2 = L.Bidirectional(L.LSTM(units=512, return_sequences=True),name=\"BiLSTM2\")(x1)\n",
        "#         x3 = L.Bidirectional(L.LSTM(units=256, return_sequences=True),name=\"BiLSTM3\")(x2)\n",
        "#         x4 = L.Bidirectional(L.LSTM(units=128, return_sequences=True),name=\"BiLSTM4\")(x3)\n",
        "\n",
        "#         z2 = L.Bidirectional(L.GRU(units=256, return_sequences=True),name=\"BiGRU1\")(x2)\n",
        "#         z3 = L.Bidirectional(L.GRU(units=128, return_sequences=True),name=\"BiGRU2\")(L.Add()([x3, z2]))\n",
        "#         z4 = L.Bidirectional(L.GRU(units=128, return_sequences=True),name=\"BiGRU3\")(L.Add()([x4, z3]))\n",
        "\n",
        "#         x = L.Concatenate(axis=2, name=\"Concat\")([x4, z2, z3, z4])\n",
        "\n",
        "#         out = L.Dense(128, activation='selu', kernel_initializer=tf.keras.initializers.lecun_normal(seed=2021))(x)\n",
        "#         out = LayerNormalization(axis=-1)(out)\n",
        "#         out = L.Dense(1)(out)\n",
        "\n",
        "#         model = tf.keras.Model(inputs=inp, outputs=out)\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# model = dnn_model(strategy)\n",
        "# model.summary()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bMNQVAsaFPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ec0afc-7201-4162-e75d-edabfa3960e8"
      },
      "source": [
        "strategy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.distribute.tpu_strategy.TPUStrategy at 0x7f0162c44f90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "5xj80dy2DyIM",
        "outputId": "58d02b75-662b-4fec-c103-bb99105988d2"
      },
      "source": [
        "# wandbを起動\n",
        "# wandb.finish()\n",
        "run = wandb.init(\n",
        "    project='kaggle-vpp',\n",
        "    name=cfg['globals']['name'],\n",
        "    config=cfg\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/okkuun/kaggle-vpp/runs/2mm3u8au\" target=\"_blank\">keras_single_11</a></strong> to <a href=\"https://wandb.ai/okkuun/kaggle-vpp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQb-m1YgB1n1"
      },
      "source": [
        "\n",
        "\n",
        "# cross-validation(group k-fold by breath_id)\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=cfg['globals']['seed'])\n",
        "# fold_dict = {'breath_id': [], 'fold': []}\n",
        "# for i, (train_breath, val_breath) in enumerate(kf.split(train['breath_id'].unique())):\n",
        "#     fold_dict['breath_id'] += val_breath.tolist()\n",
        "#     fold_dict['fold'] += np.full(len(val_breath), i).tolist()\n",
        "# fold_df = pd.DataFrame(fold_dict.values(), index=fold_dict.keys()).T\n",
        "# train = pd.merge(train, fold_df, on='breath_id')\n",
        "# train['fold'] = train['fold'].astype(int)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2IdUo0NJUp5"
      },
      "source": [
        "def GBVPP_loss(y_true, y_pred, cols = 80):\n",
        "    u_out = y_true[:, cols: ]\n",
        "    y = y_true[:, :cols ]\n",
        "\n",
        "    w = 1 - u_out\n",
        "    mae = w * tf.abs(y - y_pred)\n",
        "    return tf.reduce_sum(mae, axis=-1) / tf.reduce_sum(w, axis=-1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ejQtZt1ag4f"
      },
      "source": [
        "def smooth_l1_loss(y_true, y_pred, beta=0.1):\n",
        "    # y = y_true[:, :80]\n",
        "\n",
        "    error = y - y_pred\n",
        "    mode = tf.keras.backend.abs(error) < beta\n",
        "    squared_loss = 0.5 * tf.keras.backend.square(error) / beta\n",
        "    linear_loss = tf.keras.backend.abs(error) - 0.5 * beta\n",
        "    return tf.where(mode, squared_loss, linear_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AsB2BYWYc4N"
      },
      "source": [
        "# y_true \\in [y_true, u_out]で構成\n",
        "# y_pred \\in [pressure_in, pressure_out]で構成\n",
        "# [TODO] beta = 0.005, 0.001\n",
        "def mask_smooth_l1_loss(y_true, y_pred, cols=80):\n",
        "    u_out = y_true[:, cols: ]\n",
        "    # u_out = u_out == 0\n",
        "    pressure_true = y_true[:, :cols ]\n",
        "\n",
        "    pressure_in = y_pred[:, :cols]\n",
        "    pressure_out = y_pred[:, cols:]\n",
        "    \n",
        "    loss = smooth_l1_loss(pressure_true[u_out==0], pressure_in[u_out==0]) * 2.\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOD4q9dotX12"
      },
      "source": [
        "def mask_l1_loss(y_true, y_pred, cols=80):\n",
        "    u_out = y_true[:, cols:]\n",
        "    pressure_true = y_true[:, :cols]\n",
        "\n",
        "    return tf.keras.losses.mean_absolute_error(pressure_true[u_out == 0], y_pred[u_out == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV6bALoozPvh"
      },
      "source": [
        "def mask_l1_metrics(y_true, y_pred, cols=80):\n",
        "    u_out = y_true[:, cols:]\n",
        "    pressure_true = y_true[:, :cols]\n",
        "\n",
        "    return tf.keras.metrics.mean_absolute_error(pressure_true[u_out == 0], y_pred[u_out == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XgSy6_Zlocw"
      },
      "source": [
        "def all_l1_loss(y_true, y_pred):\n",
        "    pressure_true = y_true[:, :80]\n",
        "    return tf.keras.losses.mean_absolute_error(pressure_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXO08b0TAyQK"
      },
      "source": [
        "# main\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "# detect and init the TPU\n",
        "\n",
        "train_data = train[cfg['features']['all_features']].values.reshape(-1, 80, len(cfg['features']['all_features']))\n",
        "test_data = test[cfg['features']['all_features']].values.reshape(-1, 80, len(cfg['features']['all_features']))\n",
        "\n",
        "# gpu_strategy = tf.distribute.get_strategy()\n",
        "test_preds = []\n",
        "features = cfg['features']['all_features']\n",
        "kf = KFold(n_splits=16, shuffle=True, random_state=2021)\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_data, targets)):\n",
        "    # if fold != 0:\n",
        "    #     continue\n",
        "    K.clear_session()\n",
        "    X_train, X_valid = train_data[train_idx], train_data[valid_idx]\n",
        "    y_train, y_valid = targets[train_idx], targets[valid_idx]\n",
        "    u_out_train, u_out_valid = u_outs[train_idx], u_outs[valid_idx]\n",
        "\n",
        "    # y_train_log, y_valid_log = np.log1p(y_train), np.log1p(y_valid)\n",
        "\n",
        "    # y_train = np.concatenate([y_train, u_out_train], axis=1)\n",
        "    # y_valid = np.concatenate([y_valid, u_out_valid], axis=1)\n",
        "    \n",
        "    # model\n",
        "    with strategy.scope():\n",
        "        model = dnn_model()\n",
        "        lr = tf.keras.experimental.CosineDecayRestarts(1e-3, first_decay_steps=100 * X_train.shape[0] // 2056, t_mul=1.0, alpha=1e-3)\n",
        "    # lr = 1e-3\n",
        "        opt = tf.optimizers.Adam(learning_rate=lr)\n",
        "        # opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=1e-7)\n",
        "        # loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "        # acc_fn = GBVPP_loss()\n",
        "        # loss_fn = smooth_l1_loss\n",
        "        model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    # lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.85, patience=7, verbose=1)\n",
        "#         lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n",
        "    es = EarlyStopping(monitor=\"val_loss\", patience=95, verbose=1, mode=\"min\", restore_best_weights=True)\n",
        "#         masked_mae = MaskedValidationMAE()\n",
        "    output_path = f\"./drive/MyDrive/vpp/{cfg['globals']['name']}/\"\n",
        "    checkpoint_filepath = output_path + f\"folds{fold}.hdf5\"\n",
        "    sv = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=False, mode='auto', save_freq='epoch', options=None\n",
        "    )\n",
        "\n",
        "    # model.fit(\n",
        "    #     X_train, np.append(y_train, u_out_train, axis=1),\n",
        "    #     validation_data=(X_valid, np.append(y_valid, u_out_valid, axis=1)),\n",
        "    #     epochs=cfg['model']['epochs'], batch_size=cfg['model']['batch_size'],\n",
        "    #     callbacks=[lr, es, sv, WandbCallback()]\n",
        "    # )\n",
        "\n",
        "    histry = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        epochs=500, batch_size=BATCH_SIZE,\n",
        "        callbacks=[es, sv], shuffle=True,\n",
        "        steps_per_epoch=X_train.shape[0] // 2056, validation_steps = X_valid.shape[0] // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "    # validationで結果見る\n",
        "    pred_pressure = model.predict(X_valid, batch_size=BATCH_SIZE)\n",
        "    valid_mask = u_out_valid == 0\n",
        "    score = mae(y_valid[:, :80][valid_mask], pred_pressure.reshape(-1, 80)[valid_mask])\n",
        "    print(f\"Fold-{fold} | OOF Score: {score}\")\n",
        "\n",
        "    # test\n",
        "    pred_pressure = model.predict(test_data, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze()\n",
        "    test_preds.append(pred_pressure)\n",
        "\n",
        "    del model, X_train, X_valid, pred_pressure, y_train, y_valid\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqNkg4SemQRf"
      },
      "source": [
        "sample_sub[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
        "sample_sub[\"pressure\"] = np.round((sample_sub.pressure - P_MIN)/P_STEP) * P_STEP + P_MIN\n",
        "sample_sub[\"pressure\"] = np.clip(sample_sub.pressure, P_MIN, P_MAX)\n",
        "sample_sub.to_csv(output_path + 'submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgnpIjwdAq1q"
      },
      "source": [
        "# validationで実測値と予測値を観察"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Dlnt9IA2yj"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.rnn import LayerNormLSTMCell\n",
        "# detect and init the TPU\n",
        "\n",
        "# with tpu_strategy.scope():\n",
        "\n",
        "# gpu_strategy = tf.distribute.get_strategy()\n",
        "fold_observe = 2\n",
        "features = cfg['features']['all_features']\n",
        "kf = KFold(n_splits=7, shuffle=True, random_state=2021)\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_data, targets)):\n",
        "    if fold != fold_observe:\n",
        "        continue\n",
        "    X_train, X_valid = train_data[train_idx], train_data[valid_idx]\n",
        "    y_train, y_valid = targets[train_idx], targets[valid_idx]\n",
        "    u_out_train, u_out_valid = u_outs[train_idx], u_outs[valid_idx]\n",
        "    \n",
        "    # model\n",
        "    for fold_pred in range(7):\n",
        "        if fold_pred == fold_observe:\n",
        "            continue\n",
        "        valid_preds = []\n",
        "        output_path = f\"./drive/MyDrive/vpp/{cfg['globals']['name']}/\"\n",
        "        checkpoint_filepath = output_path + f\"folds{fold_pred}.hdf5\"\n",
        "        with tpu_strategy.scope():\n",
        "            model = keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "            valid_preds.append(model.predict(X_valid, batch_size=2048, verbose=2).squeeze().reshape(-1, 1).squeeze())\n",
        "\n",
        "        del model\n",
        "    valid_preds = np.median(np.vstack(valid_preds), axis=0)\n",
        "    valid_preds = np.round((valid_preds - P_MIN) / P_STEP) * P_STEP + P_MIN\n",
        "    valid_preds = np.clip(valid_preds, P_MIN, P_MAX)\n",
        "    if fold == fold_observe:\n",
        "        break\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG4-6QbgKxtS"
      },
      "source": [
        "valid_idx.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEaFExH7LJ7p"
      },
      "source": [
        "u_out_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oMHSUjBH8To"
      },
      "source": [
        "valid_preds.reshape(-1, 80).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po3lrkPUIhPs"
      },
      "source": [
        "train['breath_id'].unique()[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-DbVVx-A2rA"
      },
      "source": [
        "valid_preds = valid_preds.reshape(-1, 80)\n",
        "breath_id_list = train['breath_id'].unique()[valid_idx].tolist()\n",
        "mae_list = []\n",
        "for i, a_breath_id in enumerate(breath_id_list):\n",
        "    is_u_out = u_out_valid[i] == 0\n",
        "    mae_loss = mae(y_valid[i, is_u_out], valid_preds[i, is_u_out])\n",
        "    mae_list.append(mae_loss)\n",
        "\n",
        "rank_mae_valid = pd.DataFrame()\n",
        "rank_mae_valid['breath_id'] = breath_id_list\n",
        "rank_mae_valid['mae'] = mae_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsJumOnCISbv"
      },
      "source": [
        "rank_mae_valid = rank_mae_valid.sort_values('mae')\n",
        "y_valid = y_valid[rank_mae_valid.index]\n",
        "valid_preds = valid_preds[rank_mae_valid.index]\n",
        "rank_mae_valid.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FxhgY6-MGMS"
      },
      "source": [
        "rank_mae_valid.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sjB42qXMJor"
      },
      "source": [
        "rank_mae_valid['mae'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpzzpn6mPhTq"
      },
      "source": [
        "y_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E09yzoFSPjM7"
      },
      "source": [
        "y_valid[rank_mae_valid.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdzoClBFNOWx"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_one_breath_id(top=True, rank=1, show_data=False):\n",
        "    \"\"\"\n",
        "    top: Trueで上位○位、Falseで下位○位\n",
        "    rank: 順位\n",
        "    show_data: はじめのいくつかの特徴量を表示\n",
        "    \"\"\"\n",
        "    if top:\n",
        "        rank -= 1\n",
        "    else:\n",
        "        rank *= -1\n",
        "    a_rank_mae = rank_mae_valid.iloc[rank]\n",
        "    print(f\"breath_id : {a_rank_mae['breath_id']}\")\n",
        "    print(f\"MAE : {a_rank_mae['mae']}\")\n",
        "\n",
        "    a_breath_id_data = train[train['breath_id']==a_rank_mae['breath_id']]\n",
        "\n",
        "    # u_outが切り替わった瞬間をキャッチ\n",
        "    idx_change_u_out = a_breath_id_data['u_out'] - a_breath_id_data['u_out'].shift(1).fillna(0)\n",
        "    idx_change_u_out = np.arange(80)[idx_change_u_out!=0]\n",
        "    print(idx_change_u_out)\n",
        "\n",
        "    # display(a_breath_id)\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(len(a_breath_id_data)), y_valid[rank], label='True')\n",
        "    plt.plot(np.arange(len(a_breath_id_data)), valid_preds[rank], label='Pred')\n",
        "    plt.axvline(idx_change_u_out, ymax=40, color='black', linestyle='--', label='Change u_out')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if show_data:\n",
        "        display(a_breath_id_data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePW-U5EtN3e3"
      },
      "source": [
        "plot_one_breath_id(rank=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mewTAYKOCS6"
      },
      "source": [
        "plot_one_breath_id(rank=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQXhso2OFBh"
      },
      "source": [
        "plot_one_breath_id(rank=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-46gJT-OIOi"
      },
      "source": [
        "plot_one_breath_id(rank=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2dEbVFVO6Wa"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoxfPy3uQqY6"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMTPxe53QqP5"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eywA_2S8RBGa"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmqsHcc4RKAC"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZMK3iCXawmU"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtnZxH7MbxYW"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LFyoWiIbJeK"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjuM9GlXdRLa"
      },
      "source": [
        "plot_one_breath_id(top=False, rank=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM3c0C7DZuR5"
      },
      "source": [
        "# 所感\n",
        "*   スケールが大きなものほど外しやすい？\n",
        "*   全体の時系列の形は合ってそうなので、無闇にconvとかattentionとかしたところで感\n",
        "*   スケールが外しやすいものと特徴量の関係が知りたかったりする\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmKuxZm_eCRk"
      },
      "source": [
        "X_valid = X_valid[rank_mae_valid.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h90BmKKgWiW"
      },
      "source": [
        "X_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARX_adwnfuVW"
      },
      "source": [
        "valid_features = pd.DataFrame(data=X_valid.reshape(-1, len(cfg['features']['all_features'])), columns=cfg['features']['all_features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN0-KKUphenP"
      },
      "source": [
        "breath_id_list = rank_mae_valid['breath_id'].tolist()\n",
        "breath_id_np = np.vstack([breath_id_list for _ in range(80)]).T.reshape(-1)\n",
        "valid_features['breath_id'] = breath_id_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5envXpiPh5O1"
      },
      "source": [
        "# merge\n",
        "valid_df = pd.merge(rank_mae_valid, valid_features, on='breath_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYd8uh6Nj9Yg"
      },
      "source": [
        "valid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljIAF_cukS1Z"
      },
      "source": [
        "valid_df['is_huge_error'] = (valid_df['mae'] >= 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ER-93ualUO5"
      },
      "source": [
        "valid_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV_dRpKzRJxO"
      },
      "source": [
        "# スケールの違いによる\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ0ud0U8kSKn"
      },
      "source": [
        "# u_inは変換後も大きく偏っている\n",
        "# maeが大きいものは特に０から離れている（ガウス分布から離れている）印象\n",
        "plt.figure()\n",
        "plt.hist(valid_df.loc[valid_df['is_huge_error']==False, 'u_in'], color='blue', density=True, alpha=0.4)\n",
        "plt.hist(valid_df.loc[valid_df['is_huge_error']==True, 'u_in'], color='orange', density=True, alpha=0.4)\n",
        "# plt.hist(train['u_in'], color='green', density=True, alpha=0.4, bins=1000)\n",
        "plt.xlabel('u_in')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXzuCwHo9lD"
      },
      "source": [
        "# u_inの最大値と予測しにくさに関係はなさそう\n",
        "u_in_max_list = valid_df['u_in'].values.reshape(-1, 80).max(axis=1)\n",
        "mae_list = valid_df['mae'].values.reshape(-1, 80).mean(axis=1)\n",
        "plt.figure()\n",
        "plt.scatter(u_in_max_list, mae_list)\n",
        "plt.xlabel('max(u_in)')\n",
        "plt.ylabel('mae')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSxfKLYmqnjm"
      },
      "source": [
        "# Rとmaeの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.color_palette()\n",
        "\n",
        "x1 = valid_df.loc[valid_df['R_5']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "x2 = valid_df.loc[valid_df['R_20']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "x3 = valid_df.loc[valid_df['R_50']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "\n",
        "plt.violinplot([x1, x2, x3])\n",
        "plt.xticks(ticks=[1, 2, 3])\n",
        "plt.xlabel(['R_5', 'R_20', 'R_50'])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRZY6-ZBsJSc"
      },
      "source": [
        "# Cとmaeの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "\n",
        "x1 = valid_df.loc[valid_df['C_10']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "x2 = valid_df.loc[valid_df['C_20']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "x3 = valid_df.loc[valid_df['C_50']==1, 'mae'].values.reshape(-1, 80)[:, 0]\n",
        "\n",
        "plt.violinplot([x1, x2, x3])\n",
        "plt.xticks(ticks=[1, 2, 3])\n",
        "plt.xlabel(['C_10', 'C_20', 'C_50'])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4di2x5JJslCL"
      },
      "source": [
        "valid_df['pressure'] = y_valid.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdAFTZvXsswI"
      },
      "source": [
        "# Rとpressureの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "\n",
        "x1 = valid_df.loc[valid_df['R_5']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x2 = valid_df.loc[valid_df['R_20']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x3 = valid_df.loc[valid_df['R_50']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "\n",
        "plt.violinplot([x1, x2, x3])\n",
        "plt.xticks(ticks=[1, 2, 3])\n",
        "plt.xlabel(['R_5', 'R_20', 'R_50'])\n",
        "plt.ylabel('max(pressure)')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg3bheEZtFeR"
      },
      "source": [
        "# Cとpressureの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "\n",
        "x1 = valid_df.loc[valid_df['C_10']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x2 = valid_df.loc[valid_df['C_20']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x3 = valid_df.loc[valid_df['C_50']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "\n",
        "plt.violinplot([x1, x2, x3])\n",
        "plt.xticks(ticks=[1, 2, 3])\n",
        "plt.xlabel(['C_10', 'C_20', 'C_50'])\n",
        "plt.ylabel('max(pressure)')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjk2QpVEtPcU"
      },
      "source": [
        "# R__Cとpressureの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "\n",
        "x1 = valid_df.loc[valid_df['R__C_5__10']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x2 = valid_df.loc[valid_df['R__C_5__20']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x3 = valid_df.loc[valid_df['R__C_5__50']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x4 = valid_df.loc[valid_df['R__C_20__10']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x5 = valid_df.loc[valid_df['R__C_20__20']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x6 = valid_df.loc[valid_df['R__C_20__50']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x7 = valid_df.loc[valid_df['R__C_50__10']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x8 = valid_df.loc[valid_df['R__C_50__20']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "x9 = valid_df.loc[valid_df['R__C_50__50']==1, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "\n",
        "plt.violinplot([x1, x2, x3, x4, x5, x6, x7, x8, x9])\n",
        "plt.xticks(ticks=np.arange(1, 10).tolist())\n",
        "plt.xlabel(['R__C_5__10', 'R__C_5__20', 'R__C_5__50', 'R__C_20__10', 'R__C_20__20', 'R__C_20__50', 'R__C_50__10', 'R__C_50__20', 'R__C_50__50'])\n",
        "plt.ylabel('max(pressure)')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7tGW9gzkJRh"
      },
      "source": [
        "# R__Cとmaeの関係\n",
        "\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('gray')\n",
        "\n",
        "x1 = valid_df.loc[valid_df['R__C_5__10']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x2 = valid_df.loc[valid_df['R__C_5__20']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x3 = valid_df.loc[valid_df['R__C_5__50']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x4 = valid_df.loc[valid_df['R__C_20__10']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x5 = valid_df.loc[valid_df['R__C_20__20']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x6 = valid_df.loc[valid_df['R__C_20__50']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x7 = valid_df.loc[valid_df['R__C_50__10']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x8 = valid_df.loc[valid_df['R__C_50__20']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "x9 = valid_df.loc[valid_df['R__C_50__50']==1, 'mae'].values.reshape(-1, 80).max(axis=1)\n",
        "\n",
        "plt.violinplot([x1, x2, x3, x4, x5, x6, x7, x8, x9])\n",
        "plt.xticks(ticks=np.arange(1, 10).tolist())\n",
        "plt.xlabel(['R__C_5__10', 'R__C_5__20', 'R__C_5__50', 'R__C_20__10', 'R__C_20__20', 'R__C_20__50', 'R__C_50__10', 'R__C_50__20', 'R__C_50__50'])\n",
        "plt.ylabel('max(mae)')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUtdlTUu2kJ"
      },
      "source": [
        "# \n",
        "\n",
        "mae_np = valid_df.loc[:, 'mae'].values[::80]\n",
        "pressure_np = valid_df.loc[:, 'pressure'].values.reshape(-1, 80).max(axis=1)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(pressure_np, mae_np)\n",
        "plt.xlabel('max(pressure)')\n",
        "plt.ylabel('mae')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N-3L6HWvODY"
      },
      "source": [
        "np.arange(80)[::20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpVxrHQdAqv2"
      },
      "source": [
        "# ゴミ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj3cTHfEY5wg"
      },
      "source": [
        "5788160/36176"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd7LoU9Pg6C_"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi54redOg70r"
      },
      "source": [
        "keras.layers.Bidirectional(keras.layers.LSTM(1024))(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw3L_du2hkkB"
      },
      "source": [
        "inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFQJTHV0jXQb"
      },
      "source": [
        "y_train[~.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQpqFE1r5bv"
      },
      "source": [
        "pred_valid.reshape(-1, 80)[valid_mask].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txefpwbmG18"
      },
      "source": [
        "y_valid[valid_mask].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiNK22fFrz5X"
      },
      "source": [
        "valid_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2HZDF-MnO17"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4enkjajindhX"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMFeOsVuY8HP"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDAJ8y6Ulqr7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}